{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "75feb033-8356-4e39-a8dc-755c8ab46261",
   "metadata": {},
   "source": [
    "# Required packages\n",
    "!pip install vega_datasets\n",
    "!pip install keybert\n",
    "!pip install sentence-transformers\n",
    "!pip install spacy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2d4a5d7a-70cf-477b-9c25-a7ec1df94dbe",
   "metadata": {},
   "source": [
    "import http.client\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "\n",
    "import numpy as np\n",
    "import spacy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8d6abdc3-bee4-455b-840a-6cadf6f0754c",
   "metadata": {},
   "source": [
    "# inputs\n",
    "\n",
    "volume_id = 'osu.32436000578904'\n",
    "# Connect to the HTRC API\n",
    "conn = http.client.HTTPSConnection(\"tools.htrc.illinois.edu\")\n",
    "\n",
    "# Set the headers for the API request\n",
    "headers = { 'Content-Type': \"application/json\" }\n",
    "\n",
    "key_word_model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "sentences_model = SentenceTransformer('sentence-transformers/sentence-t5-base')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "42296479-cc7d-4c8c-af2a-ef1abbdd58a1",
   "metadata": {},
   "source": [
    "# Main fuctions\n",
    "\n",
    "filter_out = ['<', '>', '(', ')', '[', ']', '{', '}', '!', '@', '#', '$', '%', '^', '&', '*', '_', '+', '=', '|', '\\\\',\n",
    "              '/', '?', ',', '.', ';', ':', '\"', \"'\", ' ',\n",
    "              'IN', 'CC', 'CD', 'SYM']\n",
    "\n",
    "def retrieve_data(conn, headers, volume_id):\n",
    "    ''' Given the API connection retrieve a JSON with the data'''\n",
    "\n",
    "    # Make an API request to get information about the volumes in the workset\n",
    "    conn.request(\"GET\", f\"/ef-api/volumes/{volume_id}/pages\", headers=headers)\n",
    "\n",
    "    # Get the response and parse the data\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    \n",
    "    # Initialize variables to store total token count, total unique word count, and other metrics\n",
    "    data = json.loads(data.decode(\"utf-8\"))\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_plain_text(token_pos_count):\n",
    "    ''' Generate plain texts'''\n",
    "    token_cleaned = []\n",
    "\n",
    "    for token in token_pos_count:\n",
    "        for tag in token_pos_count[token]:\n",
    "            if tag not in filter_out:\n",
    "                token_cleaned.append(token)\n",
    "    return token_cleaned\n",
    "\n",
    "# Retrieving Data\n",
    "def get_text_per_pages(record):\n",
    "    '''' Reconstruct the text inside the pages'''\n",
    "    \n",
    "    data = {}\n",
    "    for page in record['data']['pages']:\n",
    "        if page['body']:\n",
    "            if page['body']['tokenCount'] > 15:\n",
    "\n",
    "                page_text = generate_plain_text(page['body']['tokenPosCount'])\n",
    "                if len(page_text) > 15:\n",
    "                    # Use the page for similarity analysis\n",
    "                    data[page['seq']] = ' '.join(page_text)\n",
    "                    # Use the page for similarity analysis\n",
    "                    #data.append({'page_no': page['seq'], 'text': ' '.join(page_text)})\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def tokens_per_pages_count(record):\n",
    "    ''' Create the list of words per page and how many time the word is mentioned '''\n",
    "\n",
    "    word_key_count = []\n",
    "    \n",
    "    for page in record['data']['pages']:\n",
    "        if page['body']:\n",
    "            for token, pos_count in page['body']['tokenPosCount'].items():\n",
    "                word_key_count.append({'token': token, 'page_no': page['seq'].strip(\"0\"), 'count': list(pos_count.values())[0]})\n",
    "                    \n",
    "    return word_key_count\n",
    "\n",
    "def get_keywords(text):\n",
    "    keywords = key_word_model.extract_keywords(text, top_n=10)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def get_key_words_by_page(data: dict) -> dict:\n",
    "    pages_keywords = {}\n",
    "    for page in data:\n",
    "        keywords = get_keywords(data[page])\n",
    "        pages_keywords[page] = keywords\n",
    "\n",
    "    return pages_keywords\n",
    "\n",
    "def extract_relevant_pages(data, query_embedding):\n",
    "    page_similarities = {}\n",
    "    for idx, items in enumerate(data.items()):\n",
    "        #print(f\"Page {idx} - {items[1]}\\n\")\n",
    "\n",
    "        # Calculate the similarity between the query and the pages\n",
    "        cosine_similarities = np.dot(list_embeddings[idx, :], query_embedding[0])\n",
    "        #print(cosine_similarities)\n",
    "        page_similarities[items[0]] = cosine_similarities\n",
    "    document_attributes = [\n",
    "        {'page_no': key.strip(\"0\"), 'text': value, 'key_terms': pages_keywords[key], 'score': page_similarities[key]} for\n",
    "        key, value in data.items()]\n",
    "\n",
    "    return sorted(document_attributes, key=lambda x: x['page_no'], reverse=True)\n",
    "\n",
    "def text_lemmatizer(text):\n",
    "\n",
    "    # English pipelines include a rule-based lemmatizer\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "    print(lemmatizer.mode)  # 'rule'\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    return ([token.lemma_ for token in doc])\n",
    "    # ['I', 'be', 'read', 'the', 'paper', '.']\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "58011f4d-5c8b-40b6-b8fb-7255222c6548",
   "metadata": {},
   "source": [
    "# Raw data from the API\n",
    "data = retrieve_data(conn, headers, volume_id)\n",
    "\n",
    "# Dictionary that reconstructs the text at page level\n",
    "page_dataset = get_text_per_pages(data)\n",
    "\n",
    "# List of list of token, page_no and token count per page\n",
    "count_words = tokens_per_pages_count(data)\n",
    "\n",
    "# I want to find mentions related to this query\n",
    "query_term = \"how the childrem learn?\"\n",
    "\n",
    "# Create the embedding for the query\n",
    "query_embedding = sentences_model.encode([query_term])\n",
    "\n",
    "# Create the embeddings for each page\n",
    "list_texts = list(page_dataset.values())\n",
    "list_embeddings = sentences_model.encode(list_texts)\n",
    "\n",
    "#print(list_texts)\n",
    "\n",
    "#df = pd.DataFrame(page_dataset)\n",
    "\n",
    "#df_text = df['text'].to_list()\n",
    "#df_page_no = df['page_no'].to_list()\n",
    "\n",
    "\n",
    "# Create the embeddings for each page\n",
    "list_texts = list(page_dataset.values())\n",
    "#print(list_texts)\n",
    "list_embeddings = sentences_model.encode(list_texts)\n",
    "\n",
    "# Extract the top 10 key words of each pages\n",
    "pages_keywords = get_key_words_by_page(page_dataset)\n",
    "print(pages_keywords)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13bfa96-0d23-4e22-90f3-b838c369bb8e",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ea3de872-ef7b-4716-af2d-1186b59a2977",
   "metadata": {},
   "source": [
    "# Create a dataset with the key words and the relevant page\n",
    "relevant_pages_dataset = extract_relevant_pages(page_dataset, query_embedding)\n",
    "\n",
    "relevant_pages"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa184c56-2e37-4100-8fc3-e5569e411e14",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "992f5982-864a-4fc9-8b2b-16faad62d5c3",
   "metadata": {},
   "source": [
    "df_relevant_pages = pd.DataFrame.from_records(relevant_pages) #[0:5000]\n",
    "\n",
    "# Filter out pages with lower score\n",
    "df_relevant_pages = df_relevant_pages[df_relevant_pages['score'] > 0.83] \n",
    "\n",
    "# Create a list with the relevant page\n",
    "list_relevant_pages = [page.strip(\"0\") for page in df_relevant_pages['page_no'].to_list()]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "09107cf1-b3a1-4d8f-86a2-a4fe39711697",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from_records(count_words) #[0:5000]\n",
    "\n",
    "relevant_count_words = df[df['page_no'].isin(list_relevant_pages)]\n",
    "relevant_count_words\n",
    "len(relevant_count_words)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0a8142ed-4b16-4eb8-a954-80074306efd5",
   "metadata": {},
   "source": [
    "df_relevant_pages['key_terms'].to_list()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b9ae016d-8ce9-4bce-9ea4-06f612f83b70",
   "metadata": {},
   "source": [
    "list_key_term = set([key_term[0][0] for key_term in df_relevant_pages['key_terms'].to_list()])\n",
    "\n",
    "\n",
    "relevant_count_words = df[df['token'].isin(list_key_term)]\n",
    "relevant_count_words"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ee08397f-6620-492e-9028-182c167de821",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from_records(count_words) \n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "af593d44-0fa7-42aa-af42-7d287708e335",
   "metadata": {},
   "source": [
    "# To use this visualization we should create a .csv with the following columns Keyword, Page, Count (How many times it appear per page)\n",
    "\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "#source = data.disasters.url\n",
    "\n",
    "alt.Chart(relevant_count_words).mark_circle(\n",
    "    opacity=0.8,\n",
    "    stroke='black',\n",
    "    strokeWidth=1,\n",
    "    strokeOpacity=0.4\n",
    ").encode(\n",
    "    alt.X('page_no:Q') # quantitative data\n",
    "        .title(None)\n",
    "        .scale(domain=['0', str(df['page_no'].nunique())]),\n",
    "    alt.Y('token:N') # nominal data\n",
    "        .title(None),\n",
    "        #.sort(field=\"count\", op=\"sum\", order='descending'),\n",
    "    alt.Size('count:Q')\n",
    "        .scale(range=[0, 50])\n",
    "        .title('count'),\n",
    "        #.legend(clipHeight=30, format='s'),\n",
    "    alt.Color('token:N').legend(None),\n",
    "    tooltip=[\n",
    "        #\"token:N\",\n",
    "        alt.Tooltip(\"page_no:Q\"),\n",
    "        alt.Tooltip(\"count:Q\")\n",
    "    ],\n",
    ").properties(\n",
    "    width=450,\n",
    "    height=320,\n",
    "    title=alt.Title(\n",
    "        text=\"Global Deaths from Natural Disasters (1900-2017)\",\n",
    "        subtitle=\"The size of the bubble represents the total death count per year, by type of disaster\",\n",
    "        anchor='start'\n",
    "    )\n",
    ").configure_axisY(\n",
    "    domain=False,\n",
    "    ticks=False,\n",
    "    offset=10\n",
    ").configure_axisX(\n",
    "    grid=False,\n",
    ").configure_view(\n",
    "    stroke=None\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5119438-9e41-4993-bdfb-b0d540545054",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
